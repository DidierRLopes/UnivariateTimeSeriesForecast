{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Defines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "    Imports all modules and submodules that were necessary in this study.\n",
    "    Special mention to TimeSeriesCrossValidation which was created with the purpose to be used in this thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as dr\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# This python module was developed for this thesis\n",
    "from TimeSeriesCrossValidation import splitTrain, splitTrainVal, splitTrainValTest\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "    Defines helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute MAE over an array of different horizons\n",
    "def horizonMAE(y_true, y_pred, horizonSteps):\n",
    "    ae  = abs(y_pred - y_true);\n",
    "    list_MAE = list()\n",
    "    for i in horizonSteps:\n",
    "        list_MAE.append(round(np.mean(ae[:i]), 2))\n",
    "    return list_MAE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Mean Absolute Percentage Error\n",
    "def absolute_percentage_error(y_true, y_pred):\n",
    "    if (len(y_true[y_true == 0])):\n",
    "        print(\"Division by zero!\")\n",
    "        return None;\n",
    "    else:\n",
    "        return 100*(abs((y_pred - y_true) / y_true));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute MAPE over an array of different horizons\n",
    "def horizonMAPE(y_true, y_pred, horizonSteps):\n",
    "    ape = absolute_percentage_error(y_true, y_pred);\n",
    "    if ape is not None:\n",
    "        list_MAPE = list()\n",
    "        for i in horizonSteps:\n",
    "            list_MAPE.append(round(np.mean(ape[:i]), 2))\n",
    "        return list_MAPE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revert diffLogSeries values to real scale values using initialValue (Diff(Log(x)) -> x)\n",
    "def revertLogDiff(initialValue, diffLogSeries):\n",
    "    original = initialValue\n",
    "    #for i in np.arange(len(diffLogSeries)):\n",
    "    #    original = np.append(original, original[-1]*(np.e**diffLogSeries[i]))\n",
    "    for diffLogVal in diffLogSeries:\n",
    "        original = np.append(original, original[-1]*(np.e**diffLogVal))\n",
    "    return original[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Univariate Time-Series to study\n",
    "    df: Dataframe with an index and a value column\n",
    "    title: Title of data being used\n",
    "    xlab: Label of df.index\n",
    "    ylab: Label of df.values\n",
    "    seasonal_periods: The repetition cycle\n",
    "    \n",
    "    stepsToForecast: Steps to forecast out-of-sample (and in-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsToForecast = [1, 3, 12];\n",
    "df = dr.DataReader('CPIAUCSL', \"fred\", start='1947-01-01', end='2019-09-01')\n",
    "df_train = df[['CPIAUCSL']][:-stepsToForecast[-1]].rename(columns={'CPIAUCSL': 'train'})\n",
    "df_test = df[['CPIAUCSL']][-stepsToForecast[-1]:].rename(columns={'CPIAUCSL': 'test'})\n",
    "title = 'CPIAUC'\n",
    "xlab = 'Months/Year'; \n",
    "ylab = 'Consumer Price Index';\n",
    "seasonal_periods = 12\n",
    "\n",
    "#stepsToForecast = [1, 5, 21];\n",
    "#df = pd.read_csv(\"PSI_20_Data_1992_Stooq.csv\") \n",
    "##df = pd.DataFrame(df, columns= ['Date','Close'])\n",
    "#df.set_index('Date', drop=True, inplace=True)\n",
    "#df = df.truncate(after='2019-09-28')\n",
    "#df_train = df[['Close']][:-stepsToForecast[-1]].rename(columns={'Close': 'train'})\n",
    "#df_test = df[['Close']][-stepsToForecast[-1]:].rename(columns={'Close': 'test'})\n",
    "#title = 'PSI20: Historical Closing Prices'\n",
    "#xlab = 'Business Days/Year'; \n",
    "#ylab = 'Closing Price (Euro)';\n",
    "#seasonal_periods = 5\n",
    "\n",
    "#stepsToForecast = [1, 5, 21];\n",
    "#df = dr.data.get_data_yahoo('SPY', start= '1993-01-01', end='2019-09-27')\n",
    "#df_train = df[['Close']][:-stepsToForecast[-1]].rename(columns={'Close': 'train'})\n",
    "#df_test = df[['Close']][-stepsToForecast[-1]:].rename(columns={'Close': 'test'})\n",
    "#title = 'SP500'\n",
    "#xlab = 'Business Days/Year'; \n",
    "#ylab = 'Closing Price (Dollar)';\n",
    "#seasonal_periods=5;\n",
    "\n",
    "#stepsToForecast = [1, 3, 12];\n",
    "#df = dr.DataReader(\"TRFVOLUSM227NFWA\", \"fred\", start=\"1947-01-01\", end=\"2019-09-01\")\n",
    "#df_train = df[['TRFVOLUSM227NFWA']][:-stepsToForecast[-1]].rename(columns={'TRFVOLUSM227NFWA': 'train'})\n",
    "#df_test = df[['TRFVOLUSM227NFWA']][-stepsToForecast[-1]:].rename(columns={'TRFVOLUSM227NFWA': 'test'})\n",
    "#title = \"Vehicle Miles Traveled\";\n",
    "#xlab = \"Months/Year\";\n",
    "#ylab = \"Millions of Miles\";\n",
    "#seasonal_periods = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "    The options here include:\n",
    "        - Slicing the data\n",
    "        - Standardization or Normalization\n",
    "        - Smoothing techniques\n",
    "        - Stationarity techniques\n",
    "        - Working with residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allows training data to be sliced\n",
    "series_input = df_train.values[-7*12:].copy()\n",
    "#series_input = df_train.values.copy()\n",
    "\n",
    "# Smoothing\n",
    "# gamma = 0.3\n",
    "# ExpMA = 0.0 \n",
    "# for i in range(series_input.size):\n",
    "#    ExpMA = gamma*series_input[i] + (1-gamma)*ExpMA\n",
    "#    series_input[i] = ExpMA\n",
    "\n",
    "# Select between 'Normalization', 'Standardization', 'DiffLog'\n",
    "PreProcessing_str = 'DiffLog'\n",
    "\n",
    "if (PreProcessing_str == 'Normalization'):\n",
    "    scaler = MinMaxScaler()\n",
    "    series_input = scaler.fit_transform(series_input)\n",
    "elif (PreProcessing_str == 'Standardization'):\n",
    "    scaler = StandardScaler()\n",
    "    series_input = scaler.fit_transform(series_input)\n",
    "elif (PreProcessing_str == 'DiffLog'):\n",
    "    series_input_bd = series_input # before diff\n",
    "    series_input = np.log(pd.DataFrame(series_input)).diff().dropna().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection (tune hyper-parameters)\n",
    "    In this work, the models to be used will be:\n",
    "        - Multilayer Perceptron (MLP)\n",
    "        - Recurrent Neural Network (RNN)\n",
    "        - Long Short-Term Memory (LSTM)\n",
    "        \n",
    "    Where the tunable hyper-parameters are:\n",
    "        Explicit:\n",
    "            - n_steps_input: Number of samples to be fed to the ANN\n",
    "            - n_steps_jump: Number of samples to jump in training (speed up technique)\n",
    "            - train_epochs: Number of epochs for the model to train\n",
    "        Implicit:\n",
    "            - Number of layers\n",
    "            - Number of neurons of EACH layer\n",
    "            - Activation function of EACH layer\n",
    "            - Dropout\n",
    "            - Optimization algorithm and loss function used in training\n",
    "        \n",
    "    Note: n_steps_forecast is chosen a-priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select between 'MLP', 'RNN', or 'LSTM'\n",
    "NN_str = 'LSTM'\n",
    "\n",
    "n_steps_input = 8\n",
    "n_steps_jump = 1\n",
    "\n",
    "train_epochs = 50\n",
    "\n",
    "model = Sequential()\n",
    "if (NN_str == 'MLP'):\n",
    "    model.add(Dense(50, activation='relu', input_dim=n_steps_input))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(80, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(stepsToForecast[-1], activation='linear'))\n",
    "elif (NN_str == 'RNN'):\n",
    "    model.add(SimpleRNN(100, input_shape=(n_steps_input, 1), activation='linear', return_sequences=True))    \n",
    "    model.add(SimpleRNN(50, activation='linear', return_sequences=True)) \n",
    "    model.add(SimpleRNN(25, activation='linear', return_sequences=False))\n",
    "    model.add(Dense(stepsToForecast[-1], activation='linear'))\n",
    "elif (NN_str == 'LSTM'):\n",
    "    model.add(LSTM(10,input_shape=(n_steps_input, 1), activation='linear', return_sequences=True))  \n",
    "    model.add(LSTM(25, activation='linear', return_sequences=True))\n",
    "    model.add(LSTM(50, activation='linear', return_sequences=True))\n",
    "    model.add(LSTM(30, activation='linear', return_sequences=True))\n",
    "    model.add(LSTM(20, activation='linear', return_sequences=True))\n",
    "    model.add(LSTM(15, activation='linear', return_sequences=False))\n",
    "    model.add(Dense(stepsToForecast[-1], activation='linear'))\n",
    "else:\n",
    "    print(\"This model is not recognized.\")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "    Select one of the following cross-validation methods:\n",
    "        - Forward Chaining\n",
    "    .split_train_val_test_forwardChaining(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump) \n",
    "        - K-Fold\n",
    "    .split_train_val_test_kFold(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        - Group K-Fold\n",
    "    .split_train_val_test_groupKFold(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        \n",
    "    Note: More info at https://github.com/DidierRLopes/TimeSeriesCrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cross-Validation Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/55\n",
      "2/55\n",
      "3/55\n",
      "4/55\n",
      "5/55\n",
      "6/55\n",
      "7/55\n",
      "8/55\n",
      "9/55\n",
      "10/55\n",
      "11/55\n",
      "12/55\n",
      "13/55\n",
      "14/55\n",
      "15/55\n",
      "16/55\n",
      "17/55\n",
      "18/55\n",
      "19/55\n",
      "20/55\n",
      "21/55\n",
      "22/55\n",
      "23/55\n",
      "24/55\n",
      "25/55\n",
      "26/55\n",
      "27/55\n",
      "28/55\n",
      "29/55\n",
      "30/55\n",
      "31/55\n",
      "32/55\n",
      "33/55\n",
      "34/55\n",
      "35/55\n",
      "36/55\n",
      "37/55\n",
      "38/55\n",
      "39/55\n",
      "40/55\n",
      "41/55\n",
      "42/55\n",
      "43/55\n",
      "44/55\n",
      "45/55\n",
      "46/55\n",
      "47/55\n",
      "48/55\n",
      "49/55\n",
      "50/55\n",
      "51/55\n",
      "52/55\n",
      "53/55\n",
      "54/55\n",
      "55/55\n"
     ]
    }
   ],
   "source": [
    "# splitValTest = True considers [Train, Val, Test]\n",
    "# splitValTest = False considers [Train, Val&Test]\n",
    "splitValTest = False\n",
    "# Select between 'ForwardChaining', 'KFold', or 'GroupKFold'\n",
    "CV_str = 'ForwardChaining'\n",
    "\n",
    "if (splitValTest):\n",
    "    if (CV_str == 'ForwardChaining'):\n",
    "        X_train, y_train, X_cv, y_cv, X_test, y_test = splitTrainValTest \\\n",
    "            .split_train_val_test_forwardChaining(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "    elif (CV_str == 'KFold'):\n",
    "        X_train, y_train, X_cv, y_cv, X_test, y_test = splitTrainValTest \\\n",
    "            .split_train_val_test_kFold(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "    elif (CV_str == 'GroupKFold'):\n",
    "        X_train, y_train, X_cv, y_cv, X_test, y_test = splitTrainValTest \\\n",
    "            .split_train_val_test_groupKFold(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "    else:\n",
    "        print(\"This model Cross-Validation technique is not recognized.\")\n",
    "\n",
    "    if (PreProcessing_str == 'DiffLog'):\n",
    "        if (CV_str == 'ForwardChaining'):\n",
    "            X_train_bd, y_train_bd, X_cv_bd, y_cv_bd, X_test_bd, y_test_bd = splitTrainValTest \\\n",
    "                .split_train_val_test_forwardChaining(series_input_bd, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        elif (CV_str == 'KFold'):\n",
    "            X_train_bd, y_train_bd, X_cv_bd, y_cv_bd, X_test_bd, y_test_bd = splitTrainValTest \\\n",
    "                .split_train_val_test_kFold(series_input_bd, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        elif (CV_str == 'GroupKFold'):\n",
    "            X_train_bd, y_train_bd, X_cv_bd, y_cv_bd, X_test_bd, y_test_bd = splitTrainValTest \\\n",
    "                .split_train_val_test_groupKFold(series_input_bd, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        else:\n",
    "            print(\"This model Cross-Validation technique is not recognized.\")\n",
    "else:\n",
    "    if (CV_str == 'ForwardChaining'):\n",
    "        X_train, y_train, X_cv, y_cv = splitTrainVal \\\n",
    "            .split_train_val_forwardChaining(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "    elif (CV_str == 'KFold'):\n",
    "        X_train, y_train, X_cv, y_cv = splitTrainVal \\\n",
    "            .split_train_val_kFold(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "    elif (CV_str == 'GroupKFold'):\n",
    "        X_train, y_train, X_cv, y_cv = splitTrainVal \\\n",
    "            .split_train_val_groupKFold(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "    else:\n",
    "        print(\"This model Cross-Validation technique is not recognized.\")\n",
    "\n",
    "\n",
    "    if (PreProcessing_str == 'DiffLog'):\n",
    "        if (CV_str == 'ForwardChaining'):\n",
    "            X_train_bd, y_train_bd, X_cv_bd, y_cv_bd = splitTrainVal \\\n",
    "                .split_train_val_forwardChaining(series_input_bd, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        elif (CV_str == 'KFold'):\n",
    "            X_train_bd, y_train_bd, X_cv_bd, y_cv_bd = splitTrainVal \\\n",
    "                .split_train_val_kFold(series_input_bd, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        elif (CV_str == 'GroupKFold'):\n",
    "            X_train_bd, y_train_bd, X_cv_bd, y_cv_bd = splitTrainVal \\\n",
    "                .split_train_val_groupKFold(series_input_bd, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "        else:\n",
    "            print(\"This model Cross-Validation technique is not recognized.\")\n",
    "    \n",
    "dict_mae = {} \n",
    "for ix in np.arange(len(X_train)):\n",
    "    \n",
    "    print(\"%d/%d\" % (ix+1, len(X_train)));\n",
    "    \n",
    "    if (NN_str == 'MLP'):\n",
    "        X_train[ix] = np.squeeze(X_train[ix], axis=2)\n",
    "        X_cv[ix] = np.squeeze(X_cv[ix], axis=2)\n",
    "        if (splitValTest):\n",
    "            X_test[ix] = np.squeeze(X_test[ix], axis=2)\n",
    "        if (PreProcessing_str == 'DiffLog'):\n",
    "            X_train_bd[ix] = np.squeeze(X_train_bd[ix], axis=2)\n",
    "            X_cv_bd[ix] = np.squeeze(X_cv_bd[ix], axis=2)\n",
    "            if (splitValTest):\n",
    "                X_test_bd[ix] = np.squeeze(X_test_bd[ix], axis=2)\n",
    "    y_train[ix] = np.squeeze(y_train[ix], axis=2)\n",
    "    y_cv[ix] = np.squeeze(y_cv[ix], axis=2)\n",
    "    if (splitValTest):\n",
    "        y_test[ix] = np.squeeze(y_test[ix], axis=2)\n",
    "    if (PreProcessing_str == 'DiffLog'):\n",
    "            y_train_bd[ix] = np.squeeze(y_train_bd[ix], axis=2)\n",
    "            y_cv_bd[ix] = np.squeeze(y_cv_bd[ix], axis=2)\n",
    "            if (splitValTest):\n",
    "                y_test_bd[ix] = np.squeeze(y_test_bd[ix], axis=2)\n",
    "    \n",
    "    ## ------- TRAINING MODEL -------\n",
    "    model.fit(X_train[ix], y_train[ix], epochs = train_epochs, verbose = 0);\n",
    "    \n",
    "    ## ------- TRAINING ERROR -------\n",
    "    if (PreProcessing_str in ['Normalization', 'Standardization']): \n",
    "        y_pred_train_t = scaler.inverse_transform(model.predict(X_train[ix]))\n",
    "        y_train_t = scaler.inverse_transform(y_train[ix])\n",
    "    elif (PreProcessing_str == 'DiffLog'):\n",
    "        y_pred_train_t = list()\n",
    "        y_train_t = list()\n",
    "        for i in np.arange(len(X_train[ix])):\n",
    "            if (NN_str == 'LSTM'):\n",
    "                y_pred_train_t.append(revertLogDiff(np.array([X_train_bd[ix][i][-1]]), \\\n",
    "                    np.array(model.predict(X_train[ix][i].reshape(1, n_steps_input, 1))).tolist()[0]))\n",
    "            else:\n",
    "                y_pred_train_t.append(revertLogDiff(np.array([X_train_bd[ix][i][-1]]), \\\n",
    "                        np.array(model.predict(X_train[ix][i].reshape(1, n_steps_input))).tolist()[0]))\n",
    "            y_train_t.append(revertLogDiff(np.array([y_train_bd[ix][i][0]]), y_train[ix][i]))\n",
    "    else:\n",
    "        y_pred_train_t = model.predict(X_train[ix])\n",
    "        y_train_t = y_train[ix]\n",
    "        \n",
    "    list_mae_train = list()\n",
    "    for i in np.arange(len(y_train_t)):\n",
    "        if (PreProcessing_str == 'DiffLog'):\n",
    "            mae_train = mean_absolute_error(y_train_t[i], y_pred_train_t[i])\n",
    "        else:\n",
    "            mae_train = mean_absolute_error(y_train_t[i,:], y_pred_train_t[i,:])\n",
    "        list_mae_train.append(mae_train)\n",
    "        \n",
    "    ## ------- CROSS VALIDATION ERROR -------\n",
    "    if (PreProcessing_str in ['Normalization', 'Standardization']): \n",
    "        y_pred_cv_t = scaler.inverse_transform(model.predict(X_cv[ix]))\n",
    "        y_cv_t = scaler.inverse_transform(y_cv[ix])\n",
    "    elif (PreProcessing_str == 'DiffLog'):\n",
    "        y_pred_cv_t = list()\n",
    "        y_cv_t = list()\n",
    "        for i in np.arange(len(X_cv[ix])):\n",
    "            if (NN_str == 'LSTM'):\n",
    "                y_pred_cv_t.append(revertLogDiff(np.array([X_cv_bd[ix][i][-1]]), \\\n",
    "                        np.array(model.predict(X_cv[ix][i].reshape(1, n_steps_input, 1))).tolist()[0]))\n",
    "            else:\n",
    "                y_pred_cv_t.append(revertLogDiff(np.array([X_cv_bd[ix][i][-1]]), \\\n",
    "                        np.array(model.predict(X_cv[ix][i].reshape(1, n_steps_input))).tolist()[0]))\n",
    "            y_cv_t.append(revertLogDiff(np.array([y_cv_bd[ix][i][0]]), y_cv[ix][i]))\n",
    "    else:\n",
    "        y_pred_cv_t = model.predict(X_cv[ix])\n",
    "        y_cv_t = y_cv[ix]\n",
    "\n",
    "    list_mae_cv = list()\n",
    "    for i in np.arange(len(y_cv_t)):\n",
    "        if (PreProcessing_str == 'DiffLog'):\n",
    "            mae_cv = mean_absolute_error(y_cv_t[i], y_pred_cv_t[i])\n",
    "        else:\n",
    "            mae_cv = mean_absolute_error(y_cv_t[i,:], y_pred_cv_t[i,:])\n",
    "        list_mae_cv.append(mae_cv)\n",
    "\n",
    "    if (splitValTest):\n",
    "        ## ------- TEST ERROR -------\n",
    "        if (PreProcessing_str in ['Normalization', 'Standardization']): \n",
    "            y_pred_test_t = scaler.inverse_transform(model.predict(X_test[ix]))\n",
    "            y_test_t = scaler.inverse_transform(y_test[ix])\n",
    "        elif (PreProcessing_str == 'DiffLog'):\n",
    "            y_pred_test_t = list()\n",
    "            y_test_t = list()\n",
    "            for i in np.arange(len(X_test[ix])):\n",
    "                if (NN_str == 'LSTM'):\n",
    "                    y_pred_test_t.append(revertLogDiff(np.array([X_test_bd[ix][i][-1]]), \\\n",
    "                            np.array(model.predict(X_test[ix][i].reshape(1, n_steps_input, 1))).tolist()[0]))\n",
    "                else:\n",
    "                    y_pred_test_t.append(revertLogDiff(np.array([X_test_bd[ix][i][-1]]), \\\n",
    "                            np.array(model.predict(X_test[ix][i].reshape(1, n_steps_input))).tolist()[0]))\n",
    "                y_test_t.append(revertLogDiff(np.array([y_test_bd[ix][i][0]]), y_test[ix][i]))\n",
    "        else:\n",
    "            y_pred_test_t = model.predict(X_test[ix])\n",
    "            y_test_t = y_test[ix]\n",
    "\n",
    "        list_mae_test = list()\n",
    "        for i in np.arange(len(y_test_t)):   \n",
    "            if (PreProcessing_str == 'DiffLog'):\n",
    "                mae_test = mean_absolute_error(y_test_t[i], y_pred_test_t[i])\n",
    "            else:\n",
    "                mae_test = mean_absolute_error(y_test_t[i,:], y_pred_test_t[i,:])\n",
    "            list_mae_test.append(mae_test)\n",
    "    \n",
    "        dict_mae[str(ix)] = {'train': list_mae_train, 'cv': list_mae_cv, 'test': list_mae_test}\n",
    "    else:\n",
    "        dict_mae[str(ix)] = {'train': list_mae_train, 'cv': list_mae_cv}\n",
    "    \n",
    "trainMae = np.mean([np.mean(dict_mae[str(runIx)]['train']) for runIx in np.arange(len(X_train))]);\n",
    "cvMae = np.mean([np.mean(dict_mae[str(runIx)]['cv']) for runIx in np.arange(len(X_train))]);\n",
    "if (splitValTest):\n",
    "    testMae = np.mean([np.mean(dict_mae[str(runIx)]['test']) for runIx in np.arange(len(X_train))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Performance\n",
    "    Assess Training / Cross-Validation / Test\n",
    "    DEBUG_LEVEL = 0 - Only shows STATS of ALL runs\n",
    "                  1 - Shows the average MAE through runs \n",
    "                  2 - Shows the MAE at each run at each train/cv/test set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG_LEVEL = 0\n",
    "\n",
    "print(\"STATS of ALL runs\")    \n",
    "print(\"Train avg MAE avg = %.4f\" % trainMae) \n",
    "print(\"Cross-Validation avg MAE avg = %.4f\" % cvMae)\n",
    "if (splitValTest):\n",
    "    print(\"Test avg MAE avg = %.4f\" % testMae)\n",
    "\n",
    "list_train_mae = list()\n",
    "list_cv_mae = list()\n",
    "if (splitValTest):\n",
    "    list_test_mae = list()\n",
    "for runIx in np.arange(len(X_train)):\n",
    "    if (DEBUG_LEVEL > 0):\n",
    "        print(\"\")\n",
    "        print(\"----- RUN NUMBER %d -----\" % (runIx+1))\n",
    "\n",
    "    list_train_mae.append(np.mean(dict_mae[str(runIx)]['train']))\n",
    "    if (DEBUG_LEVEL > 0):\n",
    "        print (\"Train MAE avg = %.3f\" % np.mean(dict_mae[str(runIx)]['train'])) \n",
    "        if (DEBUG_LEVEL > 1):\n",
    "            if (len(dict_mae[str(runIx)]['train'])>1):\n",
    "                print(\"Individual trains: \" + str([round(x,4) for x in dict_mae[str(runIx)]['train']]))\n",
    "    \n",
    "    list_cv_mae.append(np.mean(dict_mae[str(runIx)]['cv']))\n",
    "    if (DEBUG_LEVEL > 0):\n",
    "        print (\"Cross-Validation MAE avg = %.3f\" % np.mean(dict_mae[str(runIx)]['cv'])) \n",
    "        if (DEBUG_LEVEL > 1):\n",
    "            if (len(dict_mae[str(runIx)]['cv'])>1):\n",
    "                print(\"Individual cross-validations: \" + str([round(x,4) for x in dict_mae[str(runIx)]['cv']]))\n",
    "    \n",
    "    if (splitValTest):\n",
    "        list_test_mae.append(np.mean(dict_mae[str(runIx)]['test']))\n",
    "        if (DEBUG_LEVEL > 0):\n",
    "            print (\"Test MAE avg = %.3f\" % np.mean(dict_mae[str(runIx)]['test'])) \n",
    "            if (DEBUG_LEVEL > 1):\n",
    "                if (len(dict_mae[str(runIx)]['test'])>1):\n",
    "                    print(\"Individual tests: \" + str([round(x,4) for x in dict_mae[str(runIx)]['test']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Absolute Errors of each run\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(np.arange(len(X_train))+1, list_train_mae, 'bo', markersize=10, label = 'Train')\n",
    "plt.plot(np.arange(len(X_train))+1, list_cv_mae, 'ro', markersize=10, label = 'Cross-Validation')\n",
    "if (splitValTest):\n",
    "    plt.plot(np.arange(len(X_train))+1, list_test_mae, 'go', markersize=10, label = 'Test')\n",
    "plt.legend()\n",
    "plt.xlabel('# RUNS')\n",
    "plt.ylabel('Average Mean Absolute Error')\n",
    "plt.title('Average Mean Absolute Error at each run')\n",
    "if (len(X_train) > 500):\n",
    "    plt.xticks(np.arange(0, len(X_train), 100))\n",
    "if (len(X_train) > 250):\n",
    "    plt.xticks(np.arange(0, len(X_train), 50))\n",
    "elif (len(X_train) > 50):\n",
    "    plt.xticks(np.arange(0, len(X_train), 10))\n",
    "else:\n",
    "    plt.xticks(np.arange(1,len(X_train)+1))\n",
    "plt.xlim([0, len(X_train)+1])\n",
    "plt.grid()\n",
    "\n",
    "if (splitValTest):\n",
    "    # Plot Mean Absolute Errors of each run\n",
    "    x_bp = np.concatenate((np.repeat('Train', len(list_train_mae)), \\\n",
    "                           np.repeat('Cross-validation', len(list_cv_mae)), \n",
    "                           np.repeat('Test', len(list_test_mae))), axis = 0)\n",
    "\n",
    "    y_bp = np.concatenate((list_train_mae, list_cv_mae, list_test_mae), axis = 0)\n",
    "else:\n",
    "    # Plot Mean Absolute Errors of each run\n",
    "    x_bp = np.concatenate((np.repeat('Train', len(list_train_mae)), \\\n",
    "                           np.repeat('Cross-validation', len(list_cv_mae))), axis = 0)\n",
    "    y_bp = np.concatenate((list_train_mae, list_cv_mae), axis = 0)\n",
    "\n",
    "    \n",
    "df_bp = pd.DataFrame(data={'Data Type': x_bp, 'Mean Average Error': y_bp})\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "bplot=sns.boxplot(y='Mean Average Error', x='Data Type', data=df_bp, width=0.5)\n",
    "bplot.artists[0].set_facecolor('blue')\n",
    "bplot.artists[1].set_facecolor('red')\n",
    "plt.title('Boxplot with average Training, Cross-Validation and Testing error');\n",
    "plt.plot([-1,0], [trainMae, trainMae],'b--', lw=2, alpha=0.2)\n",
    "plt.plot([-1,1], [cvMae, cvMae],'r--', lw=2, alpha=0.2)\n",
    "if (splitValTest):\n",
    "    bplot.artists[2].set_facecolor('green')\n",
    "    plt.plot([-1,2], [testMae, testMae],'g--', lw=2, alpha=0.2)\n",
    "    plt.xlim([-1, 2.5])\n",
    "    plt.text(1.30, 1*testMae, 'Average: ' + str(round(testMae,4)), fontsize=14, color='g', fontweight='bold')\n",
    "else:\n",
    "    plt.xlim([-1, 1.5])\n",
    "plt.text(-0.70, 1*trainMae, 'Average: ' + str(round(trainMae,4)), fontsize=14, color='b', fontweight='bold')\n",
    "plt.text(0.30, 1*cvMae, 'Average: ' + str(round(cvMae,4)), fontsize=14, color='r', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform statistics on predictions\n",
    "    This aims to run several predictions in order to have a stronger argument on predictions performance\n",
    "    MAX_ITER corresponds to the amount of training & prediction used for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 4\n",
    "\n",
    "# This uses the entire training set to train\n",
    "Xtest, ytest = splitTrain.split_train(series_input, n_steps_input, stepsToForecast[-1], n_steps_jump)\n",
    "\n",
    "if (NN_str == 'MLP'):\n",
    "    Xtest = np.squeeze(Xtest, axis=2)\n",
    "ytest = np.squeeze(ytest, axis=2)\n",
    "\n",
    "real_array = np.array(df_test.values).flatten()\n",
    "\n",
    "i = 0\n",
    "list_pred = list()\n",
    "while (i < MAX_ITER):\n",
    "    \n",
    "    print(\"%d/%d\" % (i+1, MAX_ITER));\n",
    "    \n",
    "    ## ------- TRAIN MODEL WITH ALL DATA-------\n",
    "    model.fit(Xtest, ytest, epochs = train_epochs, verbose = 0);\n",
    "\n",
    "    if (NN_str == 'MLP'):\n",
    "        yhat = model.predict(series_input[-n_steps_input:].T, verbose=0)\n",
    "        yhat_inSample = model.predict(series_input[-n_steps_input-stepsToForecast[-1]:-stepsToForecast[-1]].T, verbose=0)\n",
    "    else:\n",
    "        yhat = model.predict(series_input[-n_steps_input:].reshape(1, n_steps_input, 1), verbose=0)\n",
    "        yhat_inSample = model.predict(series_input[-n_steps_input-stepsToForecast[-1]:-stepsToForecast[-1]].reshape(1, n_steps_input, 1), verbose=0)\n",
    "\n",
    "    y_pred_test = yhat.tolist()\n",
    "    y_pred_inSample = yhat_inSample.tolist()\n",
    "\n",
    "    if (PreProcessing_str in ['Normalization', 'Standardization']): \n",
    "        y_pred_test_t = scaler.inverse_transform(y_pred_test)\n",
    "        y_pred_inSample_t = scaler.inverse_transform(y_pred_inSample)\n",
    "    elif (PreProcessing_str == 'DiffLog'):\n",
    "        y_pred_test_t = revertLogDiff(df_train.values[-1], y_pred_test[0])\n",
    "        y_pred_inSample_t = revertLogDiff(df_train.values[-stepsToForecast[-1]-1], y_pred_inSample[0])\n",
    "\n",
    "    predictions_array = np.array(y_pred_test_t).flatten()\n",
    "    predictions_inSample_array = np.array(y_pred_inSample_t).flatten()\n",
    "    \n",
    "    i = i + 1\n",
    "    mae = horizonMAE(real_array, predictions_array, stepsToForecast)\n",
    "    mape = horizonMAPE(real_array, predictions_array, stepsToForecast)\n",
    "    list_pred.append((mae, mape, predictions_array, predictions_inSample_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Statistics cleaning\n",
    "    This aims to extract the performance of our NN. For that we need to select PRED_OUTL and the forecast horizon that we are predicting for stepsToForecast(PRED_IX)\n",
    "    PRED_OUTL corresponds to the number of predictions that are considered outliers\n",
    "    PRED_IX corresponds to the number steps to forecast you want to organize your data\n",
    "\n",
    "    Note that:\n",
    "        - PRED_OUTL < MAX_ITER, with both being even numbers\n",
    "        - The percentage considered as outlier is given by 100*(PRED_OUTL/MAX_ITER)\n",
    "        - 1 < PRED_IX < len(stepsToForecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_OUTL = 2\n",
    "PRED_IX = 3\n",
    "\n",
    "# Sorts elements based on longer term horizon\n",
    "list_pred.sort(key=lambda elem: elem[0][PRED_IX-1])\n",
    "list_pred_outlRemoved = list_pred[int(PRED_OUTL/2):-int(PRED_OUTL/2)]\n",
    "\n",
    "print('The MAE(MAPE) with a horizon of ' + str(stepsToForecast[PRED_IX-1]) + ' samples for ' \\\n",
    "      + str(round(100-100*(PRED_OUTL/MAX_ITER),2)) + '% (' \\\n",
    "      + str(MAX_ITER-PRED_OUTL) + '\\\\' + str(MAX_ITER) + ') of runs is: \\t'\n",
    "      + str(list_pred_outlRemoved[0][0][PRED_IX-1]) + ' (' \\\n",
    "      + str(list_pred_outlRemoved[0][1][PRED_IX-1]) + '%) and ' \\\n",
    "      + str(list_pred_outlRemoved[-1][0][PRED_IX-1]) + ' (' \\\n",
    "      + str(list_pred_outlRemoved[-1][1][PRED_IX-1]) + '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Prediction\n",
    "    This will show the plot of, within the specified confidence interval, that perform the best prediction based on the horizon samples chosen (using PRED_IX above)\n",
    "    This allows to understand what happens throughout the horizon forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.title('Real versus Predicted ' + ylab) \n",
    "plt.plot(range(0,len(real_array)+1), np.concatenate((df_train.values[-1], real_array), axis=0), 'k-o', linewidth=2)\n",
    "plt.plot(range(1,len(predictions_array)+1), list_pred_outlRemoved[0][2], color = cmap(0), linewidth=2)\n",
    "plt.plot([0,1], [df_train.values[-1][0], list_pred_outlRemoved[0][2][0]], '--', color = cmap(0), linewidth=2)\n",
    "\n",
    "plt.xlim([0,len(predictions_array)])\n",
    "plt.xticks(np.arange(0, len(predictions_array)+1, step=1))\n",
    "plt.grid(color='k', linestyle='--', linewidth=.5)\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "plt.legend(['Real value', 'Predicted value'])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.axhline(y=0, color='k', linestyle='--', linewidth=2)\n",
    "plt.plot(range(0,len(real_array-predictions_array)+1), np.concatenate((np.array([0]), real_array-list_pred_outlRemoved[0][2]), axis=0), 'g-*', linewidth=2)\n",
    "plt.xticks(np.arange(0, len(predictions_array)+1, step=1))\n",
    "plt.xlim([0,len(predictions_array)])\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel('Prediction error')\n",
    "plt.grid(color='k', linestyle='--', linewidth=.5)\n",
    "plt.show()\n",
    "\n",
    "for i in np.arange(len(stepsToForecast)):\n",
    "    print('Prediction for ' + str(stepsToForecast[i]) + ' ' + xlab + ': MAE = '\\\n",
    "          + str(list_pred_outlRemoved[0][0][i]) + ' (' + str(list_pred_outlRemoved[0][1][i]) + '%)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Forecast in-sample vs out-sample\n",
    "    This will show the plot of an in-sample vs an out-sample prediction, to understand how the NN is capable   to adapt to the seen data vs to generalize, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('In-sample and Out-of-sample Forecast Comparison') \n",
    "plt.plot(1+np.arange(stepsToForecast[-1])-stepsToForecast[-1], df_train.values[-stepsToForecast[-1]:], 'k', linewidth = 4)\n",
    "plt.plot(1+np.arange(stepsToForecast[-1])-stepsToForecast[-1], list_pred_outlRemoved[0][3], color=cmap(1), linewidth = 2 )\n",
    "plt.plot(range(1,len(predictions_array)+1), list_pred_outlRemoved[0][2], color=cmap(0), linewidth=2)\n",
    "plt.legend(['Real value', 'In-Sample Forecast', 'Out-Sample Forecast'])\n",
    "plt.plot(1+np.arange(stepsToForecast[-1]), df_test.values, 'k-o' , linewidth = 3)\n",
    "plt.plot(1+np.arange(stepsToForecast[-1]+n_steps_input)-stepsToForecast[-1]-n_steps_input, df_train.values[-n_steps_input-stepsToForecast[-1]:], 'k', linewidth = 4)\n",
    "\n",
    "plt.xlim([1-stepsToForecast[-1]-n_steps_input,+stepsToForecast[-1]])\n",
    "plt.xticks([0], '0')\n",
    "\n",
    "plt.xlabel('Past and Future Samples')\n",
    "plt.ylabel(ylab)\n",
    "\n",
    "plt.axvspan(-stepsToForecast[-1]-n_steps_input, -stepsToForecast[-1], facecolor=cmap(1), alpha=0.2)\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "plt.hlines(ymin, -n_steps_input, 0, colors=cmap(0), linewidth=10)\n",
    "plt.vlines(-n_steps_input, ymin, ymax, colors=cmap(0), linewidth=2, linestyle='--')\n",
    "plt.vlines(0, ymin, ymax, colors=cmap(0), linewidth=2, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_outlRemoved[0][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
